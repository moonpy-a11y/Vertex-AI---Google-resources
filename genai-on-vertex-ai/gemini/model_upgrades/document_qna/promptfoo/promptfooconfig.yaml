# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json

providers:  # one or more models, with optional temperature and system instructions
  - id: "vertex:gemini-1.5-flash"
  - id: "vertex:gemini-2.5-flash-preview-05-20"
    label: 2.5-flash-thinking-disabled
    config:
      generationConfig:
        temperature: 0.0
      thinking_config:
        thinking_budget: 0
  - id: "vertex:gemini-2.5-flash-preview-05-20"
    label: 2.5-flash-thinking-enabled
    config:
      generationConfig:
        temperature: 0.0
      thinking_config:
        thinking_budget: 10000

prompts:
  - file://prompt_template.txt

tests:  # Promptfoo will generate a separate prompt for each record in this dataset based on the prompt template above
  - file://dataset.jsonl

defaultTest:  # The rules for scoring each model response
  assert:
    - type: factuality   # LLM based autorater
      value: "{{answer}}"  # use the variable "answer" from dataset.jsonl as the ground truth label