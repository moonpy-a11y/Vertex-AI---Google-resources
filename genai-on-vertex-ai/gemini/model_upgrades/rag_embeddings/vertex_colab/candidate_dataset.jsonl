{"question":"What are the given examples of common challenges?","reference":"The given examples of common challenges are: \"you create a comprehensive analysis but nobody is acting on it\"; \"you ran an experiment but nobody is using the results\"; \"you built a predictive model, but the team you built it for is not using it\"; and \"you created a dashboard but nobody is looking at it\".","retrieved_context":"), or you\u2019re unsure what exactly you\u2019re supposed to deliver, it should raise a red flag. This is typically a sign that the project needs more scoping work before someone should start running with it\\nCommon Challenges and How to Address Them\\nWe talked about general frameworks to maximize impact. But how do you make actual, specific projects more impactful?\\n\\nMany times, projects fail close to the finish line. I\u2019m sure your manager would rather help protect your bandwidth than have you 1) miss your deadlines on your key projects and 2) quit eventually from frustration.\\n\\n\\nImage by author\\nDon\u2019t cry over spilled milk\\nAnother common challenge comes from the sunk cost fallacy. You invested a lot of time into a project, but it doesn\u2019t seem to be going anywhere. by sharing mock-ups and drafts for feedback) and the final product is not what they were hoping for\\nThe dashboard is complex and your users don\u2019t understand how to get what they need\\nSolution: To address #1 and #2, start with user research to understand pain points and potential use cases of the dashboard, and involve your stakeholders during development.\\n\\nWith regards to #3, a simpler dashboard that users are comfortable with beats a more advanced one that doesn\u2019t get used. you\u2019ll be in for a bad time if key stakeholders are not bought in.\\n\\nYou don\u2019t want to be in a situation where you finish the work and then realize that another team is blocking the implementation because they have concerns you didn\u2019t address. Telling your stakeholders what the result means for them will increase chances they will act on it.\\n\\n3. You built a predictive model, but the team you built it for is not using it\\nProblem: When predictive models don\u2019t get used, it\u2019s often because of a lack of trust in the model output.\\n\\nML models themselves tend to be black boxes, and if teams don\u2019t understand how the outputs were generated and whether they are reliable, they are hesitant to rely on them."}
{"question":"What are the four steps to become more impact-focused?","reference":"The four steps to become more impact-focused are: \"Step 1: Understand what impact looks like for your role and measure your success accordingly\", \"Step 2: Ensure your work solves a real business problem\", \"Step 3: Ensure there is buy-in for your work\", and \"Step 4: Focus your time on the highest-impact thing\".","retrieved_context":"I\u2019ll get into this below.\\n\\nHow can I become more impact-focused?\\nStep 1: Understand what impact looks like for your role and measure your success accordingly\\nStop thinking about productivity metrics like \u201cI launched 5 experiments\u201d or \u201cI built this model\u201d and hold yourself accountable to driving impact.\\n\\nBut what does that look like for a Data Scientist? \\n\\nIn this post I\u2019ll go over the following:\\nWhy prioritizing impact matters not just for managers, but also ICs\\nWhy focusing on impact is hard\\nHow to maximize your impact\\nHow to overcome common challenges in driving real impact\\nLet\u2019s dive in.\\n\\nGet an email whenever Torsten Walbaum publishes.\\nGet an email whenever Torsten Walbaum publishes. Impact doesn\u2019t materialize automatically, so you need to put in the final bit of work to ensure your work gets adopted. To avoid this, you\u2019ll:\\n\\nNeed to understand whose support you need, and\\nGet them onboard from the get-go\\nThis is a complex topic in itself; I\u2019ll write a separate deep dive on how to drive alignment and get buy-in from other teams in the near future.\\n\\nStep 4: Focus your time on the highest-impact thing\\nNo matter what role you\u2019re in, you\u2019re likely juggling multiple priorities. but since your long-term career trajectory is still tied to driving impact, it helps to adopt this outcome-focused mindset.\\n\\nOnce you start doing this, you\u2019ll notice more inefficiencies you can help address, or new opportunities for growth.\\n\\nStep 2: Ensure your work solves a real business problem\\nYou\u2019ll likely know this situation: Instead of approaching you with a problem, people ask you for a specific deliverable."}
{"question":"Other than this blog post, what has the author written?","reference":"The author has also written: a paper about the emergence of instabilities in large language models and a more accessible blog post on the same topic; a LLM.int8() paper; a sparse training blog post; a TPU vs GPU blog post; and a paper about k-bit inference scaling laws.","retrieved_context":"\\n\\nGet an email whenever Torsten Walbaum publishes.\\nGet an email whenever Torsten Walbaum publishes. By signing up, you will create a Medium account if you don't already\u2026\\nmedium.com\\n\\nWhy should I focus on impact; isn\u2019t that my manager\u2019s job?\\nOf course you can leave it to your manager to worry about impact. but it\u2019s what ultimately gets you promotions and new job opportunities.\\n\\nAnd isn\u2019t it nice when your work actually feels like it moves the needle?\\n\\nFor more hands-on analytics advice, consider following me here on Medium, on LinkedIn or on Substack. How to Maximize Your Impact as a Data Scientist\\n\\nOne of the hardest pills to swallow as an Individual Contributor (IC) at work is that nobody cares about the hard work you put in. They don\u2019t even care about your output; they care about the impact you drive.\\n\\nWhat\u2019s the difference? Your output is the analysis you deliver, or the lines of code you write. Your impact is the decision your analysis helps the CEO make, or the revenue the new product feature is generating. and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post. I have written a paper about the emergence of instabilities in large language models and I also written a more accessible blog post.\\n\\nThe main take-way is this: Using 8-bit instead of 16-bit makes things very unstable, but if you keep a couple of dimensions in high precision everything works just fine.\\n\\n\\nMain results from my work on 8-bit matrix multiplication for Large Language Models (LLMs). We can see that the best 8-bit baseline fails to deliver good zero-shot performance."}
{"question":"In what contexts is BERT mentioned?","reference":"It is mentioned that for BERT large during training, the input and weight matrix of any matrix multiplication fit neatly into the L2 cache of Ada (but not other Us). The author also mentions that they benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer) on the 4x RTX 2080 Ti system. They chose BERT Large inference since, from their experience, this is the deep learning model that stresses the GPU the most.","retrieved_context":"Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nFigure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nAs we can see, setting the power limit does not seriously affect performance. 47, 56, 57, 58,  1, 15, 47] | y: 58\\nThis is one training example where the input is \u201c18, 47, 56, 57, 58, 1, 15, 47\u201d and the desired output is \u201c58\u201d. This is 8 tokens of context. I benchmarked the 4x RTX 2080 Ti system shown in Figure 5 under different power limits to test this. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer). I choose BERT Large inference since, from my experience, this is the deep learning model that stresses the GPU the most. As such, I would expect power limiting to have the most massive slowdown for this model. If we print out the module names within the loop we see that they look like this:\\n\\nlm_head\\nblocks\\nblocks.layers.4\\nblocks.layers.3\\nblocks.layers.3.ln_2\\nblocks.layers.3.ln_1\\nblocks.layers.3.mha\\nblocks.layers.3.mha.resid_dropout\\nblocks.layers.3.mha.c_proj\\nblocks.layers.3.mha.attn_dropout\\nblocks.layers. n_emb)\\nFirst, we break out the dimensions of our input into variables B and T for easy handling. In sequence modeling contexts B and T are usually used as shorthand for \u201cbatch\u201d and \u201ctime\u201d dimensions. In this case, the \u201ctime\u201d dimension of our sequence is the context length.\\n\\nNext, we calculate token and position embeddings. Notice that for the position embeddings, our input is mx.arange(T) ."}
{"question":"Where is broadcasting used?","reference":"Broadcasting is used to do the < comparision between the column vector [0, 1, 2, 3] and the row vector [0, 1, 2, 3], which is one of the steps to make a causal language modelling mask. Broadcasting is also used to add pos_emb and tok_emb elementwise to get the new representations of the tokens with positional information.","retrieved_context":"Similarly, we can get a row vector using indices[None]. Then when we do the < comparison, mlx broadcasts the vectors because they have mismatching shapes so they can\u2019t be compared elementwise. Broadcasting means mlx will replicate the vectors along the lacking dimension. This results in an elementwise comparison of two (4, 4) matrices which makes sense. Side note: I recommend familiarizing yourself with the details of broadcasting by reading this, it comes up all the time when dealing with tensors. transpose([0, 2, 1])) \/ math.sqrt(self.head_size)\\n        # attn_weights.shape = (B, T, T)\\nWe create the mask with a clever broadcasting trick. Let\u2019s say our ctx_len=4 like in the diagrams above. This effectively yields a 2x speedup since the bandwidth requirements during matrix multiplication from shared memory are halved.\\n\\nFigure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed.\\nFigure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. We can have up to 32 warps = 1024 threads in a streaming multiprocessor (SM), the GPU-equivalent of a CPU core. The resources of an SM are divided up among all active warps. This means that sometimes we want to run fewer warps to have more registers\/shared memory\/Tensor Core resources per warp.\\n\\nFor both of the following examples, we assume we have the same computational resources. Since global memory is the by far the largest cycle cost for matrix multiplication with Tensor Cores, we would even have faster GPUs if the global memory latency could be reduced. We can do this by either increasing the clock frequency of the memory (more cycles per second, but also more heat and higher energy requirements) or by increasing the number of elements that can be transferred at any one time (bus width).\\n\\nMemory Bandwidth\\nFrom the previous section, we have seen that Tensor Cores are very fast."}
{"question":"What is Karpathy known for?","reference":"Karpathy A is listed as the author of the Tiny Shakespeare dataset and the creator of an 'exellent GPT from scratch tutorial'.","retrieved_context":"https:\/\/github.com\/karpathy\/char-rnn (MIT license)\\n\\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language Models are Unsupervised Multitask Learners (2019), OpenAI\\n\\n[3] Automatic Differentiation \u2014 mlx docs The only difference between our model and the real GPT-2 now is scale! Now I encourage you to experiment \u2014 try out different settings, maybe tinker with the architecture, and see how low of a loss you can achieve.\\n\\nReferences\\n[1] Karpathy A (2015).Tiny Shakespeare [Data set]. The figure is taken from Jeff Pool\u2019s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nI was working on sparse network training in my research and I also wrote a blog post about sparse training. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post.\\nFigure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.\\nFigure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types."}
{"question":"What things did Jackie remember differently to what she saw?","reference":"In her and Perry's childhood home, Jackie didn't remember having a projector room and she remembered the doorway to the kitchen being on the other side of the living room. She also remembered that she had almost killed her brother, Perry, seven hundred and thirty days ago in a car accident, when she actually did kill her brother in that accident.","retrieved_context":"And that\u2019s when, from behind me, I heard five words that made my blood run cold, \u201cJackie! Stop the fucking car!\u201d\\n\\nI was convulsing yet paralyzed. Moving as slowly as cold molasses, I rotated on the spot towards my worst nightmare, shown on 35 mm. As much as I tried to escape what Perry had just said, I did remember.\\n\\nI could hear my dad\u2019s slurring words of encouragement, \u201cCome on, Jackie. Just one drink. \u201cI think you\u2019re ready to see how this movie ends, Jackie. This is the most progress you\u2019ve made since we\u2019ve been coming here.\u201d\\n\\nI gripped my thumbs in the palms of my hands, \u201cPerry, you\u2019re freaking the fuck out of me!\u201d\\n\\nI thought my knees might buckle as my brother\u2019s face glitched, like a flash of static snow on a television set. And as such, Dr. Lasko had been appearing in the simulations as my brother Perry, the love of my life who died in the car crash, seven hundred and thirty days prior.\\n\\nDisoriented, I blinked rapidly, the vividness of the memory contrasting sharply with the sterile, geometric ceiling tiles above me.\\n\\n\u201cI don\u2019t ever want to do that again!\u201d I was venomous.\\n\\n\u201cJackie,\u201d Dr. Through the doorway where Perry stood, particles in the air danced in the projector\u2019s cone-shaped light. That telltale winding of a film reel was the only sound in the deafening quiet of this house that I no longer recognized.\\n\\nHalf of Perry\u2019s face\u2014the one with the scar\u2014was perfectly illuminated, as though he was wearing the mask from \u201cThe Phantom of the Opera\u201d. \u201cI think you\u2019re ready to see how this movie ends, Jackie."}
{"question":"What references to alcohol are there","reference":"The narrator, Jackie, had been sober for seven hundred and thirty days. Seven hundred and thirty days ago, Jackie and Perry had gotten into a car accident that almost killed the two because Jackie had been driving after drinking Bacardi Limon.\n\nJackie saw a translucent memmory of her dad sitting motionless at a table with an empty glass of Apple-Schnapple and an empty bottle of Jim Beam. Jackie also remembered he father giving her beer as a child and her mother ignoring this.\n\nShe used the death of her parents as an excuse for her alcoholism for so long, because admitting that they helped create the monster she would eventually become was like a knife to the heart. And knowing she had been too weak to conquer the addiction from her own volition just made the weapon twist in her chest.\n\nFinally, at the end of the story, Jackie licked the cracks on her lips as her eyes closed shut, imagining the oaky comfort of bourbon on her tongue. She felt herself drift, which she thought a good thing, because she needed the rest.","retrieved_context":"His face was planted on its mahogany surface. His glass of Apple-Schnapple was empty, and so was the bottle of Jim Beam beside it.\\n\\nMom floated into the living room, our warm beverages in hand and a cigarette in her mouth, \u201cKids, your father\u2019s not feeling well. Let\u2019s have our Apple-Schnapples in here.\u201d\\n\\nOh my God. All the jobs I\u2019d lost? All of his relationships I\u2019d ruined? How could he still choose me, when so often I had chosen a forty of Jack Daniels over him?\\n\\nHow could Perry still love me after I almost fucking killed him?\\n\\nPerry\u2019s gaze widened, \u201cHey! Remember when Mom would bring out those hot drinks she always made?\u201d He paused, almost as if he was searching for the right term. \u201cApple\u2026 something? As much as I tried to escape what Perry had just said, I did remember.\\n\\nI could hear my dad\u2019s slurring words of encouragement, \u201cCome on, Jackie. Just one drink. Some days, I was sure I would dive right into it and drown. Other days, I prayed to God and the Devil himself to just let me fucking drown, already.\\n\\nThat mark became permanently etched on Perry\u2019s face on the day I quit drinking, exactly seven hundred and thirty days ago. That was the day Perry screamed bloody murder at me from the passenger seat, \u201cJackie! Stop the fucking car!\u201d But my bloodstream was far too poisoned with Bacardi Limon to listen. Just one drink. It\u2019ll be our special time, just you and me.\u201d\\n\\nThe bitterness of that first sip of beer made me squirm, but sharing a \u201cspecial time\u201d with my dad\u2014and the desperate yearning that maybe he did love me, afterall\u2014was the overwhelm of the full moon swallowing me whole."}
