{"question":"What are the given examples of common challenges?","reference":"The given examples of common challenges are: \"you create a comprehensive analysis but nobody is acting on it\"; \"you ran an experiment but nobody is using the results\"; \"you built a predictive model, but the team you built it for is not using it\"; and \"you created a dashboard but nobody is looking at it\".","retrieved_context":"), or you\u2019re unsure what exactly you\u2019re supposed to deliver, it should raise a red flag. This is typically a sign that the project needs more scoping work before someone should start running with it\\nCommon Challenges and How to Address Them\\nWe talked about general frameworks to maximize impact. But how do you make actual, specific projects more impactful?\\n\\nMany times, projects fail close to the finish line. I\u2019m sure your manager would rather help protect your bandwidth than have you 1) miss your deadlines on your key projects and 2) quit eventually from frustration.\\n\\n\\nImage by author\\nDon\u2019t cry over spilled milk\\nAnother common challenge comes from the sunk cost fallacy. You invested a lot of time into a project, but it doesn\u2019t seem to be going anywhere. by sharing mock-ups and drafts for feedback) and the final product is not what they were hoping for\\nThe dashboard is complex and your users don\u2019t understand how to get what they need\\nSolution: To address #1 and #2, start with user research to understand pain points and potential use cases of the dashboard, and involve your stakeholders during development.\\n\\nWith regards to #3, a simpler dashboard that users are comfortable with beats a more advanced one that doesn\u2019t get used. You create a comprehensive analysis but nobody is acting on it\\nProblem: This is common with analyses that don\u2019t have a clear recommendation. If you simply outline the data and potential paths forward, you are expecting your audience to do all of the heavy lifting.\\n\\nSolution: Your work starts adding real value for them once you take that work off their plate. Always give a clear recommendation; you can caveat it and show alternatives in the appendix, but you need to take a stance.\\n\\n2. you\u2019ll be in for a bad time if key stakeholders are not bought in.\\n\\nYou don\u2019t want to be in a situation where you finish the work and then realize that another team is blocking the implementation because they have concerns you didn\u2019t address."}
{"question":"What are the four steps to become more impact-focused?","reference":"The four steps to become more impact-focused are: \"Step 1: Understand what impact looks like for your role and measure your success accordingly\", \"Step 2: Ensure your work solves a real business problem\", \"Step 3: Ensure there is buy-in for your work\", and \"Step 4: Focus your time on the highest-impact thing\".","retrieved_context":"I\u2019ll get into this below.\\n\\nHow can I become more impact-focused?\\nStep 1: Understand what impact looks like for your role and measure your success accordingly\\nStop thinking about productivity metrics like \u201cI launched 5 experiments\u201d or \u201cI built this model\u201d and hold yourself accountable to driving impact.\\n\\nBut what does that look like for a Data Scientist? To avoid this, you\u2019ll:\\n\\nNeed to understand whose support you need, and\\nGet them onboard from the get-go\\nThis is a complex topic in itself; I\u2019ll write a separate deep dive on how to drive alignment and get buy-in from other teams in the near future.\\n\\nStep 4: Focus your time on the highest-impact thing\\nNo matter what role you\u2019re in, you\u2019re likely juggling multiple priorities. \\n\\nIn this post I\u2019ll go over the following:\\nWhy prioritizing impact matters not just for managers, but also ICs\\nWhy focusing on impact is hard\\nHow to maximize your impact\\nHow to overcome common challenges in driving real impact\\nLet\u2019s dive in.\\n\\nGet an email whenever Torsten Walbaum publishes.\\nGet an email whenever Torsten Walbaum publishes. but since your long-term career trajectory is still tied to driving impact, it helps to adopt this outcome-focused mindset.\\n\\nOnce you start doing this, you\u2019ll notice more inefficiencies you can help address, or new opportunities for growth.\\n\\nStep 2: Ensure your work solves a real business problem\\nYou\u2019ll likely know this situation: Instead of approaching you with a problem, people ask you for a specific deliverable. Start obsessing about response times and satisfaction scores.\\nYou don\u2019t have to be solely responsible for something in order to take (partial) credit for it. If you provided the analysis that resulted in a pricing change that saved the company millions, then you deserve part of the credit for that impact.\\n\\nYou might not feel the consequences of missing these downstream targets as immediately as your stakeholders, but since your long-term career trajectory is still tied to driving impact, it helps to adopt this outcome-focused mindset."}
{"question":"Other than this blog post, what has the author written?","reference":"The author has also written: a paper about the emergence of instabilities in large language models and a more accessible blog post on the same topic; a LLM.int8() paper; a sparse training blog post; a TPU vs GPU blog post; and a paper about k-bit inference scaling laws.","retrieved_context":"but it\u2019s what ultimately gets you promotions and new job opportunities.\\n\\nAnd isn\u2019t it nice when your work actually feels like it moves the needle?\\n\\nFor more hands-on analytics advice, consider following me here on Medium, on LinkedIn or on Substack. \\n\\nGet an email whenever Torsten Walbaum publishes.\\nGet an email whenever Torsten Walbaum publishes. By signing up, you will create a Medium account if you don't already\u2026\\nmedium.com\\n\\nWhy should I focus on impact; isn\u2019t that my manager\u2019s job?\\nOf course you can leave it to your manager to worry about impact. The figure is taken from Jeff Pool\u2019s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.\\nI was working on sparse network training in my research and I also wrote a blog post about sparse training. The cost\/performance numbers form the core of the blog post and the content surrounding it explains the details of what makes up GPU performance.\\n\\n(2) If you worry about specific questions, I have answered and addressed the most common questions and misconceptions in the later part of the blog post.\\n\\n(3) If you want to get an in-depth understanding of how GPUs, caches, and Tensor Cores work, the best is to read the blog post from start to finish. and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post."}
{"question":"In what contexts is BERT mentioned?","reference":"It is mentioned that for BERT large during training, the input and weight matrix of any matrix multiplication fit neatly into the L2 cache of Ada (but not other Us). The author also mentions that they benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer) on the 4x RTX 2080 Ti system. They chose BERT Large inference since, from their experience, this is the deep learning model that stresses the GPU the most.","retrieved_context":"Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nAs we can see, setting the power limit does not seriously affect performance. 5 MB L2\\nAmpere (RTX 30s series): 128 kb shared memory \/ 6 MB L2\\nAda (RTX 40s series): 128 kb shared memory \/ 72 MB L2\\nWe see that Ada has a much larger L2 cache allowing for larger tile sizes, which reduces global memory access. For example, for BERT large during training, the input and weight matrix of any matrix multiplication fit neatly into the L2 cache of Ada (but not other Us). As such, the slowdowns reported here are probably close to the maximum slowdowns that you can expect. The results are shown in Figure 7.\\n\\nFigure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).\\nFigure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer). I choose BERT Large inference since, from my experience, this is the deep learning model that stresses the GPU the most. As such, I would expect power limiting to have the most massive slowdown for this model. As such, the slowdowns reported here are probably close to the maximum slowdowns that you can expect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.\\nYou might ask, \u201cDoesn\u2019t this slow down the GPU?\u201d Yes, it does, but the question is by how much. I benchmarked the 4x RTX 2080 Ti system shown in Figure 5 under different power limits to test this. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer)."}
{"question":"Where is broadcasting used?","reference":"Broadcasting is used to do the < comparision between the column vector [0, 1, 2, 3] and the row vector [0, 1, 2, 3], which is one of the steps to make a causal language modelling mask. Broadcasting is also used to add pos_emb and tok_emb elementwise to get the new representations of the tokens with positional information.","retrieved_context":"__init__()\\n        self.head_size = head_size\\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\\n        indices = mx.arange(ctx_len)\\n        mask = indices[:, None] < indices[None] # broadcasting trick\\n        self. arange(ctx_len)\\n        mask = indices[:, None] < indices[None] # broadcasting trick\\n        self._causal_mask = mask * -1e9\\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\\n        self.attn_dropout = nn.Dropout(dropout)\\n        self.resid_dropout = nn.Dropout(dropout)\\n    def __call__(self, x):\\n        B, T, C = x. Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\\n        indices = mx.arange(ctx_len)\\n        mask = indices[:, None] < indices[None] # broadcasting trick\\n        self. Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\\n        indices = mx.arange(ctx_len)\\n        mask = indices[:, None] < indices[None] # broadcasting trick\\n        self. head_size, bias=False)\\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\\n        indices = mx.arange(ctx_len)\\n        mask = indices[:, None] < indices[None] # broadcasting trick\\n        self._causal_mask = mask * -1e9\\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\\n        self."}
{"question":"What is Karpathy known for?","reference":"Karpathy A is listed as the author of the Tiny Shakespeare dataset and the creator of an 'exellent GPT from scratch tutorial'.","retrieved_context":"https:\/\/github.com\/karpathy\/char-rnn (MIT license)\\n\\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language Models are Unsupervised Multitask Learners (2019), OpenAI\\n\\n[3] Automatic Differentiation \u2014 mlx docs He, too, was connected to a plethora of suction cups and wires. He looked a little worn out in the fluorescent overhead lighting. Ever since I was hospitalized and later incarcerated, Dr. Lasko had been helping me delve into my memories, namely the ones that were too excruciating for me to face. And as such, Dr. The only difference between our model and the real GPT-2 now is scale! Now I encourage you to experiment \u2014 try out different settings, maybe tinker with the architecture, and see how low of a loss you can achieve.\\n\\nReferences\\n[1] Karpathy A (2015).Tiny Shakespeare [Data set]. Lasko approached my bedside and took a seat. He treaded lightly. \u201cJackie, I understand how challenging this is for you, but you did an incredible job today. If we continue making progress like this, there's a real possibility you'll gain your freedom sooner.\u201d\\n\\nI looked at the well-meaning doctor, but all I could see was Perry. Multicolored confetti fell softly around him like that first November snow. His face was the sun. \\\"Memories are quite something, aren\u2019t they?\\\" The slight shift in his tone made my skin crawl. Perry was always wistful, but this felt different, almost\u2026 clinical, \\\"We often remember things in ways that are\u2026 easier for us to digest.\\\"\\n\\nI was fidgety. \u201cGees Perry. You sound like Dr. Lasko.\u201d\\n\\nHe seemed to enjoy my little joke.\\n\\nDr."}
{"question":"What things did Jackie remember differently to what she saw?","reference":"In her and Perry's childhood home, Jackie didn't remember having a projector room and she remembered the doorway to the kitchen being on the other side of the living room. She also remembered that she had almost killed her brother, Perry, seven hundred and thirty days ago in a car accident, when she actually did kill her brother in that accident.","retrieved_context":"Lasko started.\\n\\n\u201cDon\u2019t start,\u201d I pulled up four fingers for air quotes, \u201c\u2018Jackie, don\u2019t give up. This was the closest you\u2019ve ever come to facing the truth.\u201d\\n\\nAs the initial burst of adrenal and cortisol left my body, I fell back on my pillow. I was depleted. Quiet rivers flowed down my cheeks.\\n\\nRemoving his own suction cups, Dr. Lasko approached my bedside and took a seat. He treaded lightly. As much as I tried to escape what Perry had just said, I did remember.\\n\\nI could hear my dad\u2019s slurring words of encouragement, \u201cCome on, Jackie. Just one drink. It\u2019ll be our special time, just you and me.\u201d\\n\\nThe bitterness of that first sip of beer made me squirm, but sharing a \u201cspecial time\u201d with my dad\u2014and the desperate yearning that maybe he did love me, afterall\u2014was the overwhelm of the full moon swallowing me whole. And as such, Dr. Lasko had been appearing in the simulations as my brother Perry, the love of my life who died in the car crash, seven hundred and thirty days prior.\\n\\nDisoriented, I blinked rapidly, the vividness of the memory contrasting sharply with the sterile, geometric ceiling tiles above me.\\n\\n\u201cI don\u2019t ever want to do that again!\u201d I was venomous.\\n\\n\u201cJackie,\u201d Dr. Lasko approached my bedside and took a seat. He treaded lightly. \u201cJackie, I understand how challenging this is for you, but you did an incredible job today. If we continue making progress like this, there's a real possibility you'll gain your freedom sooner.\u201d\\n\\nI looked at the well-meaning doctor, but all I could see was Perry. Multicolored confetti fell softly around him like that first November snow. His face was the sun. And that\u2019s when, from behind me, I heard five words that made my blood run cold, \u201cJackie! Stop the fucking car!\u201d\\n\\nI was convulsing yet paralyzed. Moving as slowly as cold molasses, I rotated on the spot towards my worst nightmare, shown on 35 mm."}
{"question":"What references to alcohol are there","reference":"The narrator, Jackie, had been sober for seven hundred and thirty days. Seven hundred and thirty days ago, Jackie and Perry had gotten into a car accident that almost killed the two because Jackie had been driving after drinking Bacardi Limon.\n\nJackie saw a translucent memmory of her dad sitting motionless at a table with an empty glass of Apple-Schnapple and an empty bottle of Jim Beam. Jackie also remembered he father giving her beer as a child and her mother ignoring this.\n\nShe used the death of her parents as an excuse for her alcoholism for so long, because admitting that they helped create the monster she would eventually become was like a knife to the heart. And knowing she had been too weak to conquer the addiction from her own volition just made the weapon twist in her chest.\n\nFinally, at the end of the story, Jackie licked the cracks on her lips as her eyes closed shut, imagining the oaky comfort of bourbon on her tongue. She felt herself drift, which she thought a good thing, because she needed the rest.","retrieved_context":"Some days, I was sure I would dive right into it and drown. Other days, I prayed to God and the Devil himself to just let me fucking drown, already.\\n\\nThat mark became permanently etched on Perry\u2019s face on the day I quit drinking, exactly seven hundred and thirty days ago. That was the day Perry screamed bloody murder at me from the passenger seat, \u201cJackie! Stop the fucking car!\u201d But my bloodstream was far too poisoned with Bacardi Limon to listen. His face was planted on its mahogany surface. His glass of Apple-Schnapple was empty, and so was the bottle of Jim Beam beside it.\\n\\nMom floated into the living room, our warm beverages in hand and a cigarette in her mouth, \u201cKids, your father\u2019s not feeling well. Let\u2019s have our Apple-Schnapples in here.\u201d\\n\\nOh my God. I was only a child, and much like how my mom turned a blind eye to my father\u2019s drinking, she did the exact same when it came to her daughter.\\n\\nI\u2019d used the death of my parents as the excuse for my alcoholism for so long, because admitting that they helped create the monster I would eventually become was like a knife to the heart. And knowing I had been too weak to conquer the addiction from my own volition just made the weapon twist in my chest.\\n\\nThe room was spinning. As much as I tried to escape what Perry had just said, I did remember.\\n\\nI could hear my dad\u2019s slurring words of encouragement, \u201cCome on, Jackie. Just one drink. It\u2019ll be our special time, just you and me.\u201d\\n\\nThe bitterness of that first sip of beer made me squirm, but sharing a \u201cspecial time\u201d with my dad\u2014and the desperate yearning that maybe he did love me, afterall\u2014was the overwhelm of the full moon swallowing me whole. Stop the fucking car!\u201d But my bloodstream was far too poisoned with Bacardi Limon to listen. All I remember next was my vehicle being wrapped around a tree. I could have died that day, but what truly disturbed me in the middle of the night was the fact that I almost killed Perry.\\n\\nA lot can happen in seven hundred and thirty days. But I assure you, forgiving yourself isn\u2019t one of them.\\n\\n\u201cWell?"}
