{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZtfr2Gyx_qM"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"as is\" basis,\n",
        "# without warranties or conditions of any kind, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbBTb55fx_qN"
      },
      "source": [
        "\n",
        "## **Multi-turn Chat Eval Recipe**\n",
        "\n",
        "This Eval Recipe demonstrates how to compare quality of chat responses from two versions of Gemini using [Vertex AI Evaluation Service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjNAklK4x_qN"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-eval-multiturn-chat&utm_medium=aRT-clicks&utm_campaign=eval-multiturn-chat&destination=eval-multiturn-chat&url=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fgemini%2Fmodel_upgrades%2Fmultiturn_chat%2Fvertex_colab%2Fmultiturn_chat_eval.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-eval-multiturn-chat&utm_medium=aRT-clicks&utm_campaign=eval-multiturn-chat&destination=eval-multiturn-chat&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fcolab%2Fimport%2Fhttps%3A%252F%252Fraw.githubusercontent.com%252FGoogleCloudPlatform%252Fapplied-ai-engineering-samples%252Fmain%252Fgenai-on-vertex-ai%252Fgemini%252Fmodel_upgrades%252Fmultiturn_chat%252Fvertex_colab%252Fmultiturn_chat_eval.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-eval-multiturn-chat&utm_medium=aRT-clicks&utm_campaign=eval-multiturn-chat&destination=eval-multiturn-chat&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fworkbench%2Fdeploy-notebook%3Fdownload_url%3Dhttps%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fmain%2Fgenai-on-vertex-ai%2Fgemini%2Fmodel_upgrades%2Fmultiturn_chat%2Fvertex_colab%2Fmultiturn_chat_eval.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/vertex_colab/multiturn_chat_eval.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYtBxd6y-Ju2"
      },
      "source": [
        "- Use case: multi-turn conversation (Chat)\n",
        "\n",
        "- Metric: this eval uses a Pairwise Autorater (LLM Judge) to compare the quality of model responses.\n",
        "\n",
        "- Evaluation Dataset is a subset of [Multi-turn Prompts Dataset](https://www.kaggle.com/datasets/softageai/multi-turn-prompts-dataset). Each record in the dataset.jsonl file links to a JSON file with the history of conversation between the User and the Model. This dataset does not include any ground truth labels.\n",
        "\n",
        "Step 1 of 4: Configure all necessary parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZnEA6GZ-kMW"
      },
      "outputs": [],
      "source": [
        "%%writefile .env\n",
        "PROJECT_ID=your-project-id            # Google Cloud Project ID\n",
        "LOCATION=us-central1                  # Region for all required Google Cloud services\n",
        "EXPERIMENT_NAME=eval-multiturn-chat   # Creates Vertex AI Experiment to track the eval runs\n",
        "MODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model\n",
        "MODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model\n",
        "DATASET_URI=\"gs://gemini_assets/multiturn_chat/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage\n",
        "PROMPT_TEMPLATE_URI=gs://gemini_assets/multiturn_chat/prompt_template.txt  # Text file in Google Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqeOCM7k9t5h"
      },
      "source": [
        "Step 2 of 4: Install all required Python libraries if not already installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bR0rvHA3Lby6"
      },
      "outputs": [],
      "source": [
        "try: # Skip installation and kernel restart if this cell has been executed.\n",
        "  import dotenv\n",
        "except:\n",
        "  %pip install --upgrade --user --quiet python-dotenv google-genai google-cloud-aiplatform[evaluation]\n",
        "  import IPython\n",
        "  # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\n",
        "  IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OUfM5Lz9_aM"
      },
      "source": [
        "Step 3 of 4: Authenticate to Google Cloud (requires permission to open a popup window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRFZFC6OLby7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import vertexai\n",
        "from dotenv import load_dotenv\n",
        "from google.cloud import storage\n",
        "\n",
        "load_dotenv(override=True)\n",
        "if os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n",
        "    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQvekvLt9SWD"
      },
      "source": [
        "Step 4 of 4: Run the eval on both models and compare the Accuracy scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfG9JG9VHaNw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from google import genai\n",
        "from google.genai.types import Content, Part\n",
        "from IPython.display import clear_output\n",
        "from vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples\n",
        "from vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory\n",
        "\n",
        "def load_file(gcs_uri: str) -> str:\n",
        "    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n",
        "    return blob.download_as_string().decode('utf-8')\n",
        "\n",
        "def load_dataset(dataset_uri: str):\n",
        "    jsonl = load_file(dataset_uri)\n",
        "    samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]\n",
        "    df = pd.DataFrame(samples)\n",
        "    df['history'] = df['chat_uri'].apply(lambda document_uri: load_file(document_uri))\n",
        "    return df[['history']]\n",
        "\n",
        "def generate_chat_responses(project_id: str, location:str, model: str, dataset: pd.DataFrame, response_column_name: str) -> None:\n",
        "    '''Generate the final model response for each conversation in the dataset using the specified model.'''\n",
        "    client = genai.Client(vertexai=True, project=project_id, location=location)\n",
        "    responses = []\n",
        "    user_prompts = []\n",
        "    for i, record in dataset.iterrows():\n",
        "        print(f'Generating chat completion #{i+1} with {model}')\n",
        "        messages = json.loads(record.get('history'))\n",
        "        last_user_message = messages.pop()\n",
        "        history = [\n",
        "            Content(\n",
        "                role=message['role'],\n",
        "                parts=[Part(text=message['content'])],\n",
        "            )\n",
        "            for message in messages\n",
        "        ]\n",
        "        chat = client.chats.create(model=model, history=history)\n",
        "        response = chat.send_message(message=[Part(text=last_user_message['content'])])\n",
        "        user_prompts.append(last_user_message)\n",
        "        responses.append( response.candidates[0].content.parts[0].text )\n",
        "    dataset['prompt'] = user_prompts  # The last user message is required by the Autorater\n",
        "    dataset[response_column_name] = responses\n",
        "    print(f'{len(responses)} responses from model {model} are stored in dataset column \"{response_column_name}\"')\n",
        "\n",
        "def run_eval(project_id: str, location:str, experiment_name: str, baseline_model: str, candidate_model: str, dataset_uri: str):\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "    timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n",
        "    dataset=load_dataset(dataset_uri)\n",
        "    generate_chat_responses(project_id, location, baseline_model, dataset, 'baseline_model_response')\n",
        "    generate_chat_responses(project_id, location, candidate_model, dataset, 'response')\n",
        "    task = EvalTask(\n",
        "        dataset=dataset,\n",
        "        metrics=[MetricPromptTemplateExamples.Pairwise.MULTI_TURN_CHAT_QUALITY],\n",
        "        experiment=experiment_name\n",
        "    )\n",
        "    eval_results = task.evaluate(\n",
        "        experiment_run_name=f\"{timestamp}-{baseline_model.replace('.', '-')}\"\n",
        "    )\n",
        "    clear_output()\n",
        "    print(f\"Baseline model win rate: {eval_results.summary_metrics['pairwise_multi_turn_chat_quality/baseline_model_win_rate']:.2f}\")\n",
        "    print(f\"Candidate model win rate: {eval_results.summary_metrics['pairwise_multi_turn_chat_quality/candidate_model_win_rate']:.2f}\")\n",
        "\n",
        "run_eval(\n",
        "    project_id=os.getenv('PROJECT_ID'),\n",
        "    location=os.getenv('LOCATION'),\n",
        "    experiment_name=os.getenv('EXPERIMENT_NAME'),\n",
        "    baseline_model=os.getenv('MODEL_BASELINE'),\n",
        "    candidate_model=os.getenv('MODEL_CANDIDATE'),\n",
        "    dataset_uri=os.getenv('DATASET_URI')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhZZ030LJGL3"
      },
      "source": [
        "You can access all prompts and model responses in `candidate_results.metrics_table`\n",
        "\n",
        "Please use our [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) to learn about all available metrics and customization options."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "multiturn_chat_eval.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
